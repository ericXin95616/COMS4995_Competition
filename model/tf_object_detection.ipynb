{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ12suCaL6-D"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1-DUUqJLcwu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Gb8J2KMFAc"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSTvT0l9zcj3"
      },
      "source": [
        "Check GPU availability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1SKhM-gNZTw"
      },
      "source": [
        "Install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZkNfIhhNboc"
      },
      "outputs": [],
      "source": [
        "!pip3 install tensorflow-gpu==2.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb8b4tPvN60C"
      },
      "source": [
        "Set up object-detection api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYWmtUFCN9SY"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snu-ovyDPFjN"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEyV_lyFPTSf"
      },
      "source": [
        "Set up cocoapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK0Wg9dnPVTN"
      },
      "outputs": [],
      "source": [
        "!pip install cython\n",
        "!git clone https://github.com/cocodataset/cocoapi.git\n",
        "%cd ./cocoapi/PythonAPI\n",
        "!ls\n",
        "!make\n",
        "!cp -r pycocotools ../../models/research/\n",
        "%cd ../../models/research\n",
        "!ls | grep pycocotools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq73kZvNQRO8"
      },
      "source": [
        "Compile protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpkr8mZ1QUDW"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KLXcm6VQ1H_"
      },
      "source": [
        "Install object_detection api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xluDzkB7Q43b"
      },
      "outputs": [],
      "source": [
        "%cd /content/models/research\n",
        "!pwd\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuuH-5Q7RgnS"
      },
      "source": [
        "Test to see if object detection api is successfully installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVcSWhDTR979"
      },
      "outputs": [],
      "source": [
        "# should only executed once\n",
        "#!pip install tensorflow==2.7.0\n",
        "#!pip install tf-models-official==2.7.0\n",
        "#!pip install tensorflow_io==0.23.1\n",
        "!pip install opencv-python-headless==4.1.2.30\n",
        "!pip install tensorflow==2.7.0\n",
        "!pip install tensorboard~=2.6.0\n",
        "!pip install tensorflow-estimator~=2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIV0JqCMRka4"
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "!pip list | grep tensor\n",
        "%cd /content/models/research\n",
        "!python3 object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoKBesOcTmbZ"
      },
      "source": [
        "Download the models and retrain them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ2y1zKGatl_"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!ls drive/MyDrive/COMS4995_Competition/object_detection_workspace/models\n",
        "# cd ../.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4RGDswesqR"
      },
      "source": [
        "Fine-tuning the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHEbGCGn2TpR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(\"Found GPU at: {}\".format(device_name))\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d38Nmjow-xVr",
        "outputId": "5d7aac8f-26dd-4a2a-cc76-bf6256ad9ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 37 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 3,139 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 7s (60.5 MB/s)\n",
            "(Reading database ... 155335 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 155313 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"
          ]
        }
      ],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6WYrMAmRevwt",
        "outputId": "04859e9e-e1e0-4bcc-ac6c-8bfa80fc1f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensorboard                   2.6.0\n",
            "tensorboard-data-server       0.6.1\n",
            "tensorboard-plugin-wit        1.8.1\n",
            "tensorflow                    2.7.0\n",
            "tensorflow-addons             0.16.1\n",
            "tensorflow-datasets           4.0.1\n",
            "tensorflow-estimator          2.7.0\n",
            "tensorflow-gcs-config         2.8.0\n",
            "tensorflow-gpu                2.7.0\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io                 0.24.0\n",
            "tensorflow-io-gcs-filesystem  0.24.0\n",
            "tensorflow-metadata           1.7.0\n",
            "tensorflow-model-optimization 0.7.2\n",
            "tensorflow-probability        0.16.0\n",
            "tensorflow-text               2.8.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "/content/drive/MyDrive/COMS4995_Competition/object_detection_workspace\n",
            "2022-03-21 21:05:57.101287: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "W0321 21:05:57.106386 139650093176704 cross_device_ops.py:1387] There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "I0321 21:05:57.107557 139650093176704 mirrored_strategy.py:376] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I0321 21:05:57.567462 139650093176704 config_util.py:552] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0321 21:05:57.567674 139650093176704 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "I0321 21:05:57.578831 139650093176704 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0321 21:05:57.578966 139650093176704 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 224\n",
            "I0321 21:05:57.579025 139650093176704 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 7\n",
            "I0321 21:05:57.582995 139650093176704 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0321 21:05:57.614365 139650093176704 efficientnet_model.py:144] round_filter input=32 output=48\n",
            "I0321 21:05:57.614515 139650093176704 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0321 21:05:57.784641 139650093176704 efficientnet_model.py:144] round_filter input=16 output=24\n",
            "I0321 21:05:57.784818 139650093176704 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0321 21:05:58.248146 139650093176704 efficientnet_model.py:144] round_filter input=24 output=32\n",
            "I0321 21:05:58.248326 139650093176704 efficientnet_model.py:144] round_filter input=40 output=56\n",
            "I0321 21:05:58.725548 139650093176704 efficientnet_model.py:144] round_filter input=40 output=56\n",
            "I0321 21:05:58.725756 139650093176704 efficientnet_model.py:144] round_filter input=80 output=112\n",
            "I0321 21:05:59.421203 139650093176704 efficientnet_model.py:144] round_filter input=80 output=112\n",
            "I0321 21:05:59.421391 139650093176704 efficientnet_model.py:144] round_filter input=112 output=160\n",
            "I0321 21:06:00.098169 139650093176704 efficientnet_model.py:144] round_filter input=112 output=160\n",
            "I0321 21:06:00.098348 139650093176704 efficientnet_model.py:144] round_filter input=192 output=272\n",
            "I0321 21:06:01.007140 139650093176704 efficientnet_model.py:144] round_filter input=192 output=272\n",
            "I0321 21:06:01.007312 139650093176704 efficientnet_model.py:144] round_filter input=320 output=448\n",
            "I0321 21:06:01.235207 139650093176704 efficientnet_model.py:144] round_filter input=1280 output=1792\n",
            "I0321 21:06:01.282518 139650093176704 efficientnet_model.py:454] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0321 21:06:01.514981 139650093176704 deprecation.py:347] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['data/clean_haze_train.record']\n",
            "I0321 21:06:01.747888 139650093176704 dataset_builder.py:163] Reading unweighted datasets: ['data/clean_haze_train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['data/clean_haze_train.record']\n",
            "I0321 21:06:01.748254 139650093176704 dataset_builder.py:80] Reading record datasets for input file: ['data/clean_haze_train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0321 21:06:01.748342 139650093176704 dataset_builder.py:81] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0321 21:06:01.748396 139650093176704 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W0321 21:06:01.750677 139650093176704 deprecation.py:347] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0321 21:06:01.771535 139650093176704 deprecation.py:347] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0321 21:06:09.583301 139650093176704 deprecation.py:347] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:465: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0321 21:06:14.170203 139650093176704 deprecation.py:347] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:465: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2022-03-21 21:06:17.866139: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:414: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
            "2022-03-21 21:07:11.834700: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W0321 21:07:12.344187 139643397654272 deprecation.py:551] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "W0321 21:07:25.806250 139643397654272 utils.py:80] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "W0321 21:07:44.155207 139643397654272 utils.py:80] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "W0321 21:08:01.938680 139643397654272 utils.py:80] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "W0321 21:08:21.129422 139643397654272 utils.py:80] Gradients do not exist for variables ['stack_6/block_1/expand_bn/gamma:0', 'stack_6/block_1/expand_bn/beta:0', 'stack_6/block_1/depthwise_conv2d/depthwise_kernel:0', 'stack_6/block_1/depthwise_bn/gamma:0', 'stack_6/block_1/depthwise_bn/beta:0', 'stack_6/block_1/project_bn/gamma:0', 'stack_6/block_1/project_bn/beta:0', 'top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "INFO:tensorflow:Step 4900 per-step time 9.651s\n",
            "I0321 21:23:16.920212 139650093176704 model_lib_v2.py:707] Step 4900 per-step time 9.651s\n",
            "INFO:tensorflow:{'Loss/classification_loss': nan,\n",
            " 'Loss/localization_loss': nan,\n",
            " 'Loss/regularization_loss': nan,\n",
            " 'Loss/total_loss': nan,\n",
            " 'learning_rate': 0.07998715}\n",
            "I0321 21:23:16.920554 139650093176704 model_lib_v2.py:708] {'Loss/classification_loss': nan,\n",
            " 'Loss/localization_loss': nan,\n",
            " 'Loss/regularization_loss': nan,\n",
            " 'Loss/total_loss': nan,\n",
            " 'learning_rate': 0.07998715}\n",
            "INFO:tensorflow:Step 5000 per-step time 8.183s\n",
            "I0321 21:36:55.259777 139650093176704 model_lib_v2.py:707] Step 5000 per-step time 8.183s\n",
            "INFO:tensorflow:{'Loss/classification_loss': nan,\n",
            " 'Loss/localization_loss': nan,\n",
            " 'Loss/regularization_loss': nan,\n",
            " 'Loss/total_loss': nan,\n",
            " 'learning_rate': 0.07998606}\n",
            "I0321 21:36:55.260084 139650093176704 model_lib_v2.py:708] {'Loss/classification_loss': nan,\n",
            " 'Loss/localization_loss': nan,\n",
            " 'Loss/regularization_loss': nan,\n",
            " 'Loss/total_loss': nan,\n",
            " 'learning_rate': 0.07998606}\n",
            "INFO:tensorflow:Step 5100 per-step time 8.006s\n",
            "I0321 21:50:15.833422 139650093176704 model_lib_v2.py:707] Step 5100 per-step time 8.006s\n",
            "INFO:tensorflow:{'Loss/classification_loss': nan,\n",
            " 'Loss/localization_loss': nan,\n",
            " 'Loss/regularization_loss': nan,\n",
            " 'Loss/total_loss': nan,\n",
            " 'learning_rate': 0.07998492}\n",
            "I0321 21:50:15.833767 139650093176704 model_lib_v2.py:708] {'Loss/classification_loss': nan,\n",
            " 'Loss/localization_loss': nan,\n",
            " 'Loss/regularization_loss': nan,\n",
            " 'Loss/total_loss': nan,\n",
            " 'learning_rate': 0.07998492}\n",
            "INFO:tensorflow:Step 5200 per-step time 8.015s\n",
            "I0321 22:03:37.322837 139650093176704 model_lib_v2.py:707] Step 5200 per-step time 8.015s\n",
            "INFO:tensorflow:{'Loss/classification_loss': nan,\n",
            " 'Loss/localization_loss': nan,\n",
            " 'Loss/regularization_loss': nan,\n",
            " 'Loss/total_loss': nan,\n",
            " 'learning_rate': 0.07998374}\n",
            "I0321 22:03:37.323421 139650093176704 model_lib_v2.py:708] {'Loss/classification_loss': nan,\n",
            " 'Loss/localization_loss': nan,\n",
            " 'Loss/regularization_loss': nan,\n",
            " 'Loss/total_loss': nan,\n",
            " 'learning_rate': 0.07998374}\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep tensor\n",
        "!pip install --upgrade numpy\n",
        "%cd /content/drive/MyDrive/COMS4995_Competition/object_detection_workspace\n",
        "!python model_main_tf2.py --pipeline_config_path=models/efficientdet_d4_coco17_tpu-32/v1/pipeline.config --model_dir=models/efficientdet_d4_coco17_tpu-32/v1/ --checkpoint_every_n=100 --num_workers=1 alsologtostderr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ilzT71JhZhE"
      },
      "source": [
        "Use retrained models to generate labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "fpMurrPVeBjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6dfec9b-b274-476d-f578-3ef2c8403561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COMS4995_Competition\n",
            "center_net {\n",
            "  num_classes: 1\n",
            "  feature_extractor {\n",
            "    type: \"hourglass_104\"\n",
            "    channel_means: 104.01361846923828\n",
            "    channel_means: 114.03422546386719\n",
            "    channel_means: 119.91659545898438\n",
            "    channel_stds: 73.60276794433594\n",
            "    channel_stds: 69.89082336425781\n",
            "    channel_stds: 70.91507720947266\n",
            "    bgr_ordering: true\n",
            "  }\n",
            "  image_resizer {\n",
            "    keep_aspect_ratio_resizer {\n",
            "      min_dimension: 512\n",
            "      max_dimension: 512\n",
            "      pad_to_max_dimension: true\n",
            "    }\n",
            "  }\n",
            "  object_detection_task {\n",
            "    task_loss_weight: 1.0\n",
            "    offset_loss_weight: 1.0\n",
            "    scale_loss_weight: 0.10000000149011612\n",
            "    localization_loss {\n",
            "      l1_localization_loss {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  object_center_params {\n",
            "    object_center_loss_weight: 1.0\n",
            "    classification_loss {\n",
            "      penalty_reduced_logistic_focal_loss {\n",
            "        alpha: 2.0\n",
            "        beta: 4.0\n",
            "      }\n",
            "    }\n",
            "    min_box_overlap_iou: 0.699999988079071\n",
            "    max_box_predictions: 100\n",
            "  }\n",
            "}\n",
            "\n",
            "Running inference for ./AOD-Net/data/result/145.jpg... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:179: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n",
            "Running inference for ./AOD-Net/data/result/new_031.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/165.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_021.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_029.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_041.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_068.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_042.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_049.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/095.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_025.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_033.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_047.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_040.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_027.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_043.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/122.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_061.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_062.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_058.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/007.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_036.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_045.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_052.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_026.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_050.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_055.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_023.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_051.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_065.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_060.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_032.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_039.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_038.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_037.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/049.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/177.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_056.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_066.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/178.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_067.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_057.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_030.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_064.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/163.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_054.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_034.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_059.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_046.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_024.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_048.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/097.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_028.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/029.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_035.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/186.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_063.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_053.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_044.jpg... Done\n",
            "Running inference for ./AOD-Net/data/result/new_020.jpg... Done\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "After preparing the TFRecord, we fine-tunes pretrained model using TFRecords.\n",
        "Load the fine-tuned models, try to use it to detect vehicle in the image.\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "python model_main_tf2.py --pipeline_config_path=models/faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8/v1/pipeline.config \n",
        "--model_dir=models/faster_rcnn_inception_resnet_v2_640x640_coco17_tpu-8/v1/ --checkpoint_every_n=100 --num_workers=1 alsologtostderr\n",
        "\"\"\"\n",
        "%cd /content/drive/MyDrive/COMS4995_Competition/\n",
        "import os\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "\n",
        "PATH_TO_MODEL_DIR = './object_detection_workspace/models/centernet_hg104_512x512_coco17_tpu-8/v2'\n",
        "PATH_TO_CFG = PATH_TO_MODEL_DIR + '/pipeline.config'\n",
        "PATH_TO_CKPT = PATH_TO_MODEL_DIR + '/checkpoints'\n",
        "PATH_TO_LABELS = './object_detection_workspace/data/label_map.pbtxt'\n",
        "\n",
        "PATH_TO_DATASET_DIR = './BPP_train'\n",
        "PATH_TO_VALIDATION_DIR = PATH_TO_DATASET_DIR + '/haze_test'\n",
        "PATH_TO_TRAIN_DIR = PATH_TO_DATASET_DIR + '/haze_train'\n",
        "PATH_TO_CLEAN_VALIDATION_DIR = PATH_TO_DATASET_DIR + '/dehaze_test'\n",
        "PATH_TO_CLEAN_TRAIN_DIR = PATH_TO_DATASET_DIR + '/dehaze_train'\n",
        "PATH_TO_RESULT_LABEL_DIR = './results'\n",
        "PATH_TO_DRYRUN_DIR = './dry-run-1'\n",
        "\n",
        "\n",
        "validation_images = glob.glob(PATH_TO_VALIDATION_DIR + '/*.jpg')\n",
        "train_images = glob.glob(PATH_TO_TRAIN_DIR + '/*.jpg')\n",
        "clean_validation_images = glob.glob(PATH_TO_CLEAN_VALIDATION_DIR + '/*.jpg')\n",
        "clean_train_images = glob.glob(PATH_TO_CLEAN_TRAIN_DIR + '/*.jpg')\n",
        "dry_run_images = glob.glob(PATH_TO_DRYRUN_DIR + '/*.jpg')\n",
        "dehaze_dry_run_images = glob.glob('./AOD-Net/data/result/*.jpg')\n",
        "\n",
        "\n",
        "# Load pipeline config and build a detection model\n",
        "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
        "model_config = configs['model']\n",
        "print(model_config)\n",
        "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-35')).expect_partial()\n",
        "im_width = 1845\n",
        "im_height = 750\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def detect_fn(image):\n",
        "    \"\"\"Detect objects in image.\"\"\"\n",
        "\n",
        "    image, shapes = detection_model.preprocess(image)\n",
        "    prediction_dict = detection_model.predict(image, shapes)\n",
        "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
        "    # print(detections)\n",
        "\n",
        "    return detections\n",
        "\n",
        "'''\n",
        "test_im_path = train_images[0]\n",
        "hazy_im, _ = get_hazy_clean_image(test_im_path)\n",
        "tensor_im = tf.convert_to_tensor(hazy_im)\n",
        "converted_img = tf.image.convert_image_dtype(tensor_im, tf.float32)[tf.newaxis, ...]\n",
        "detections = detect_fn(converted_img)\n",
        "# detections = detection_model(converted_img)\n",
        "result = {key: value.numpy() for key, value in detections.items()}\n",
        "'''\n",
        "\n",
        "def filter_coords(result, threshold=0.0):\n",
        "    '''\n",
        "      A helper method which extracts the bounding boxes and threshold confidence scores\n",
        "      from the data structure returned from the pretrained model\n",
        "\n",
        "      Parameters:\n",
        "              result : Output of the pretrained tensor flow model\n",
        "              threshold : Optional threshold confidence above which bounding boxes are selected\n",
        "      Returns:\n",
        "              The list of coordinates of the detected objects and the threshold score\n",
        "    '''\n",
        "    class_entities = result['detection_classes']\n",
        "    detection_scores = result['detection_scores']\n",
        "    bounding_boxes = result['detection_boxes']\n",
        "    detection_boxes_with_thresholds = []\n",
        "    for detection_class, detection_score, bounding_box in zip(class_entities, detection_scores, bounding_boxes):\n",
        "        if 1 == detection_class and detection_score >= threshold:\n",
        "            detection_boxes_with_thresholds.append((bounding_box, detection_score))\n",
        "    return detection_boxes_with_thresholds\n",
        "\n",
        "\n",
        "# f = filter_coords(result, threshold=0.5)\n",
        "'''\n",
        "coords = result['detection_boxes']\n",
        "width = 750\n",
        "height = 1845\n",
        "converted_coords = []\n",
        "print(coords)\n",
        "for coord in coords[0]:\n",
        "    print(coord)\n",
        "    ymin, xmin, ymax, xmax = coord\n",
        "    converted_coord = [int(xmin * width), int(ymin * height), int(xmax * width), int(ymax * height)]\n",
        "    converted_coords.append(converted_coord)\n",
        "\n",
        "print(converted_coords)\n",
        "visualize_bb_image(val_images[0], converted_coords)\n",
        "'''\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "    \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "    Puts image into numpy array to feed into tensorflow graph.\n",
        "    Note that by convention we put it into a numpy array with shape\n",
        "    (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "    Args:\n",
        "      path: the file path to the image\n",
        "\n",
        "    Returns:\n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    \"\"\"\n",
        "    return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "for image_path in dehaze_dry_run_images:\n",
        "\n",
        "    print('Running inference for {}... '.format(image_path), end='')\n",
        "\n",
        "    image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "    # Things to try:\n",
        "    # Flip horizontally\n",
        "    # image_np = np.fliplr(image_np).copy()\n",
        "\n",
        "    # Convert image to grayscale\n",
        "    # image_np = np.tile(\n",
        "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "\n",
        "    detections = detect_fn(input_tensor)\n",
        "\n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                  for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "\n",
        "    # detection_classes should be ints.\n",
        "    label_id_offset = 1\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64) + label_id_offset\n",
        "\n",
        "    image_np_with_detections = image_np.copy()\n",
        "\n",
        "    # visualize the detected box on the image\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "            image_np_with_detections,\n",
        "            detections['detection_boxes'],\n",
        "            detections['detection_classes'],\n",
        "            detections['detection_scores'],\n",
        "            category_index,\n",
        "            use_normalized_coordinates=True,\n",
        "            max_boxes_to_draw=10,\n",
        "            min_score_thresh=0.3,\n",
        "            agnostic_mode=False)\n",
        "\n",
        "    plt.figure()\n",
        "    filename = '{}.jpg'.format(Path(image_path).stem)\n",
        "    plt.imshow(image_np_with_detections)\n",
        "    plt.savefig('results_fig/'+filename)\n",
        "\n",
        "    # output the detected box to the file\n",
        "    coords = filter_coords(detections, threshold=0.3)\n",
        "    newfile = os.path.join(PATH_TO_RESULT_LABEL_DIR, Path(image_path).stem + '.txt')\n",
        "    with open(newfile, \"w+\") as f:\n",
        "        for coord in coords:\n",
        "            ymin, xmin, ymax, xmax = coord[0]\n",
        "            threshold = coord[1]\n",
        "            format_string = f'vehicle {xmin * im_width} {ymin * im_height} {xmax * im_width} {ymax * im_height} {threshold}\\n'\n",
        "            f.write(format_string)\n",
        "    print('Done')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Copy_tf_object_detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}